//! Question and answer task using a language model.
//!
//! This module provides document-based question answering capabilities using
//! a language model. Given a document context and a question, the model
//! generates an answer based solely on the information present in the document.

use serde::{Deserialize, Serialize};

use crate::inference::error::InferenceError;
use crate::inference::phi4mini::Phi4MiniInference;

/// The result of a question and answer inference operation.
///
/// Contains the original question, the generated answer, and an optional
/// confidence score indicating the model's certainty in the response.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QuestionAndAnswerResult {
    /// The original question that was asked.
    pub question: String,
    /// The answer generated by the model based on the document context.
    pub answer: String,
    /// Optional confidence score for the answer (0.0 to 1.0).
    ///
    /// This field is not currently populated by the model but is reserved
    /// for future use when confidence estimation is implemented.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub confidence: Option<f32>,
}

impl QuestionAndAnswerResult {
    /// Creates a new `QuestionAndAnswerResult` with the given question and answer.
    ///
    /// # Arguments
    ///
    /// * `question` - The original question that was asked.
    /// * `answer` - The answer generated by the model.
    ///
    /// # Returns
    ///
    /// A new `QuestionAndAnswerResult` with no confidence score.
    pub fn new(question: String, answer: String) -> Self {
        Self {
            question,
            answer,
            confidence: None,
        }
    }
}

/// Task handler for document-based question answering.
///
/// This struct provides methods for answering questions based on document
/// content using the Phi-4 Mini language model. The model is instructed to
/// answer questions using only information present in the provided context,
/// avoiding hallucinations or external knowledge.
///
/// # Architecture
///
/// The task uses [`Phi4MiniInference`] as the underlying language model,
/// with a carefully crafted system prompt that instructs the model to:
/// - Answer based only on the provided document context
/// - Indicate when information is not found in the document
/// - Be concise and quote relevant portions when appropriate
pub struct QuestionAndAnswerTask;

impl QuestionAndAnswerTask {
    /// Initializes the underlying Phi-4 Mini model if not already loaded.
    ///
    /// This method should be called at application startup to preload the
    /// language model, avoiding latency on the first question. The model
    /// is cached globally and reused for subsequent calls.
    ///
    /// # Errors
    ///
    /// Returns an [`InferenceError`] if:
    /// - The model files cannot be found or loaded
    /// - The tokenizer fails to initialize
    /// - There is insufficient memory to load the model
    pub fn get_or_init() -> Result<(), InferenceError> {
        Phi4MiniInference::get_or_init()
    }

    /// Answers a question based on the provided document context.
    ///
    /// This method sends the context and question to the Phi-4 Mini model,
    /// which generates an answer using only information found in the context.
    /// The model is instructed to indicate when information is not available
    /// rather than making assumptions.
    ///
    /// # Arguments
    ///
    /// * `context` - The document text to use as context for answering.
    ///   This should contain the relevant content from the analyzed document.
    /// * `question` - The question to answer. Should be a clear, specific
    ///   question about the document content.
    ///
    /// # Returns
    ///
    /// Returns a [`QuestionAndAnswerResult`] containing the original question
    /// and the model's answer on success.
    ///
    /// # Errors
    ///
    /// Returns an [`InferenceError`] if:
    /// - The context is empty or contains only whitespace
    /// - The question is empty or contains only whitespace
    /// - The model fails to generate a response
    /// - The model returns an empty response
    pub fn answer(
        context: &str,
        question: &str,
    ) -> Result<QuestionAndAnswerResult, InferenceError> {
        if context.trim().is_empty() {
            return Err(InferenceError::PreprocessingError {
                operation: "validate input".to_string(),
                message: "Context cannot be empty".to_string(),
            });
        }

        if question.trim().is_empty() {
            return Err(InferenceError::PreprocessingError {
                operation: "validate input".to_string(),
                message: "Question cannot be empty".to_string(),
            });
        }

        let tokenizer = Phi4MiniInference::get_tokenizer()?;
        let system_message = Self::build_system_message();
        let prompt = format!("Document Context:\n{context}\n\nQuestion: {question}");

        let response = Phi4MiniInference::with_instance(|model| {
            model.generate(&prompt, tokenizer, Some(&system_message))
        })?;

        Self::parse_response(&response, question)
    }

    /// Builds the system prompt for the language model.
    ///
    /// The system message instructs the model to act as a document analysis
    /// expert, answering questions based solely on the provided context.
    /// It emphasizes avoiding hallucinations and clearly indicating when
    /// information cannot be found.
    fn build_system_message() -> String {
        r#"You are a document analysis expert. Answer questions based ONLY on the information provided in the document context.

INSTRUCTIONS:
1. Answer the question based ONLY on the information in the document context
2. If the answer is not in the document, respond with "I cannot find this information in the provided document."
3. Be concise and direct in your answer
4. Quote relevant parts of the document when appropriate
5. Do not make assumptions or add information not present in the document
6. If the question is ambiguous, provide the most reasonable interpretation based on the context

Provide your answer directly without additional formatting or explanation."#.to_string()
    }

    /// Parses the model's raw response into a structured result.
    ///
    /// # Arguments
    ///
    /// * `response` - The raw text response from the language model.
    /// * `question` - The original question, included in the result.
    ///
    /// # Returns
    ///
    /// Returns a [`QuestionAndAnswerResult`] on success.
    ///
    /// # Errors
    ///
    /// Returns an [`InferenceError::PredictionError`] if the model returned
    /// an empty response.
    fn parse_response(
        response: &str,
        question: &str,
    ) -> Result<QuestionAndAnswerResult, InferenceError> {
        let answer = response.trim().to_string();

        if answer.is_empty() {
            return Err(InferenceError::PredictionError {
                operation: "parse response".to_string(),
                message: "Model returned an empty response".to_string(),
            });
        }

        Ok(QuestionAndAnswerResult::new(question.to_string(), answer))
    }
}
